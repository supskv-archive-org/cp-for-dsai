{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP - Midterm - 2020\n",
    "## Instruction\n",
    "* Modify this file to be Midterm-<Your FirstName-[First Letter of Last Name]>, e.g., Midterm-Chaklam-S.ipynb\n",
    "* This exam accounts for 25% of the overall course assessment.\n",
    "* This exam is open-booked; open-internet.\n",
    "* You ARE NOT allowed to use sklearn or any libraries, unless stated.\n",
    "* The completed exams shall be submitted at the Google Classroom\n",
    "* All code should be complemented with comments, unless it's really obvious. I and Joe reserve the privilege to give you zero for any part of the question where the benefit of doubt is not justified\n",
    "\n",
    "## Examination Rules:\n",
    "* For offline students, you may leave the room temporarily with the approval and supervision of the proctors. No extra time will be added to the exam in such cases.\n",
    "* For online students, you are required to turn on your webcam during the entire period of the exam time\n",
    "* Students will be allowed to leave at the earliest 45 minutes after the exam has started\n",
    "* All work should belong to you. A student should NOT engage in the following activities which proctors reserve the right to interpret any of such act as academic dishonesty without questioning:\n",
    "* Chatting with any human beings physically or via online methods\n",
    "* Plagiarism of any sort, i.e., copying from internet sources or friends. Both copee and copier shall be given a minimum penalty of zero mark for that particular question or the whole exam.\n",
    "No make-up exams are allowed. Special considerations may be given upon a valid reason on unpredictable events such as accidents or serious sickness.\n",
    "\n",
    "## Question 1 (21 pts)\n",
    "\n",
    "1). **The rabbit**: (5pts)\n",
    "\n",
    "Once upon a time, there is a father rabbit lives in a far away jungle. Everyday, the father rabbit has to go out and find some carrots for his family. In his family there are mother rabbit, grampa rabbit, sister rabbit, and his son. In total there are 5 rabbits to feed. In one day, the adult rabbits (himself, mother rabbit and sister rabbit) will eat 3 carrots while the elderly eat 2 carrots and baby rabbit eat 1 carrot.\n",
    "\n",
    "Unfortunately, the carrots are not easy to find. The father rabbit has to travel into the scary jungle and find some carrot then bring them back to the family before the sunset at 6PM.\n",
    "\n",
    "- Every 1 km, the rabbit will find 3 carrots.\n",
    "- The rabbit will use 1 hour to travel 1 km.\n",
    "\n",
    "In summary, in order to find the least number of carrot for each day, the rabbit will have to use (3 + 3 + 3 + 2 + 1)/3 =  4 hours. This mean that he has to leave the house at the latest 10AM (4 hours for go out and another 4 for comming back).\n",
    "\n",
    "This daily work has to be done exactly on time, leaving to late will cause whether his life or his family life. Would you like to help the rabbit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "# print 'yes' to help the rabbit or 'no' to refuse the challenge. (if yes -> 1 pt)\n",
    "# Your code here\n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to hear that young programmer!!\n",
    "\n",
    "What I have in mind is to build a clock that when the rabbit puts the number of (adult, elderly, young) rabbit, it will calculate how many hours is required for a travel. Of cause we have to make it as a function because the number of each rabbit type will change over the time.\n",
    "\n",
    "- Write a function <code>carrot</code> that takes three integers as an input in follow this format <code>(adult, elderly, young)</code> (1pt)\n",
    "- The function will calculate number of hour required for travelling a day. (1pt)\n",
    "- The function will also calcualte the time to leave. Think of it as an alarm clock for leaving the house (1pt)\n",
    "- The function will return a tuple (\\#hours, #time) (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "def carrot(adult, elderly, young):\n",
    "    carrot_per_km = 3\n",
    "    hour_per_km = 1\n",
    "    end_of_travel = 18\n",
    "    need_carrot = (adult * 3) + (elderly * 2) + young\n",
    "    need_km = need_carrot / carrot_per_km\n",
    "    need_hour = round((need_km / hour_per_km) * 2)\n",
    "    taken = end_of_travel - need_hour\n",
    "    return need_hour, taken\n",
    "\n",
    "carrot(3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **Print the shape**: (16pts)\n",
    "\n",
    "- Write a function <code>square</code> that takes integer as an input. (1pt)\n",
    "- The function will return a string of * in the shape of a square with both width and height equal to the input interger. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLevel: 3\\n***\\n***\\n***\\n\\nLevel: 5\\n*****\\n*****\\n*****\\n*****\\n*****\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Level: 3\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "Level: 5\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "*****\n",
      "*****\n",
      "*****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def square(n):\n",
    "    for i in range(n):\n",
    "        print('*' * n)\n",
    "\n",
    "square(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function <code>triangle</code> that takes integer as an input. (1pt)\n",
    "- The function will return a string of * in the shape of a triangle with level equal to the input integer. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLevel: 3\\n  *\\n **\\n***\\n\\nLevel: 5\\n    *\\n   **\\n  ***\\n ****\\n*****\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Level: 3\n",
    "  *\n",
    " **\n",
    "***\n",
    "\n",
    "Level: 5\n",
    "    *\n",
    "   **\n",
    "  ***\n",
    " ****\n",
    "*****\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    *\n",
      "   **\n",
      "  ***\n",
      " ****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def triangle(n):\n",
    "    for i in range(1, n+1):\n",
    "        print(' ' * (n-i) + '*' * (i))\n",
    "\n",
    "triangle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function <code>pyramid</code> that takes integer as an input. (1pt)\n",
    "- The function will return the string of * in the shape of a pyramid with level equal to the input interger. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nLevel: 3 \\n  * \\n ***\\n*****\\n\\nLevel: 5\\n    *\\n   ***\\n  *****\\n *******\\n*********\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Level: 3 \n",
    "  * \n",
    " ***\n",
    "*****\n",
    "\n",
    "Level: 5\n",
    "    *\n",
    "   ***\n",
    "  *****\n",
    " *******\n",
    "*********\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    *    \n",
      "   ***   \n",
      "  *****  \n",
      " ******* \n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def pyramid(n):\n",
    "    for i in range(1, n+1):\n",
    "        pad = ' ' * (n-i)\n",
    "        print(pad + '*' * (i*2-1) + pad)\n",
    "\n",
    "pyramid(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the three algorithms into one single class.\n",
    "- Create a class named <code>MyShape</code> that can do the followings\n",
    "    - Take two arguments during the class construction. The first one is an integer and the second one is a string. The names are <code>level</code> and <code>shape</code> (1pt)\n",
    "    - Check the input arguments whether the interger is in the range of \\[1,10\\] and string is in the set of {'squ','tri','pyr'}. Raise a <code>ValueError</code>. (2pts)\n",
    "    - Both attributes should be able to change via a set method **only**. <code>set\\[attrName\\]</code> (1pt)\n",
    "    - Of cause, the set method should check the out of range too. (1pt)\n",
    "    - To check the current setting, write a get method. <code>get\\[attrName\\]</code> (1pt)\n",
    "    - Print the shape with method <code>show</code>. It should return the string of the current shape with the correct level (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample 1\\n\\n>>> ms = MyShape(2,\\'tri\\')\\n>>> ms.show()\\n *\\n**\\n>>> ms.setLevel(3)\\n>>> ms.setShape(\\'squ\\')\\n>>> ms.show()\\n***\\n***\\n***\\n>>> ms.setShape(\\'a\\')\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nValueError: ..........\\n>>>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Example 1\n",
    "\n",
    ">>> ms = MyShape(2,'tri')\n",
    ">>> ms.show()\n",
    " *\n",
    "**\n",
    ">>> ms.setLevel(3)\n",
    ">>> ms.setShape('squ')\n",
    ">>> ms.show()\n",
    "***\n",
    "***\n",
    "***\n",
    ">>> ms.setShape('a')\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "ValueError: ..........\n",
    ">>>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "***\n",
      "***\n",
      "  *\n",
      " **\n",
      "***\n",
      "    *    \n",
      "   ***   \n",
      "  *****  \n",
      " ******* \n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "class MyShape:\n",
    "    def __init__(self, level, shape):\n",
    "        if (not (level >= 1 and level <= 10)):\n",
    "            raise ValueError('Level must contain in 1 to 10.') \n",
    "        if (shape not in ['squ','tri','pyr']):\n",
    "            raise ValueError('Shape must be squ, tri, or pyr.') \n",
    "        self._level = level\n",
    "        self._shape = shape\n",
    "    def setLevel(self, v):\n",
    "        if (not (v >= 1 and v <= 10)):\n",
    "            raise ValueError('Level must contain in 1 to 10.') \n",
    "        self._level = v\n",
    "    def setShape(self, v):\n",
    "        if (v not in ['squ','tri','pyr']):\n",
    "            raise ValueError('Shape must be squ, tri, or pyr.') \n",
    "        self._shape = v\n",
    "    def getLevel(self):\n",
    "        return self._level\n",
    "    def getShape(self):\n",
    "        return self._shape\n",
    "    def show(self):\n",
    "        level = self._level\n",
    "        method = self._shape\n",
    "        if (method == 'squ'):\n",
    "            square(level)\n",
    "        elif (method == 'tri'):\n",
    "            triangle(level)\n",
    "        elif (method == 'pyr'):\n",
    "            pyramid(level)\n",
    "        else:\n",
    "            raise ValueError('Shape is invalid.')\n",
    "\n",
    "m = MyShape(3, 'squ')\n",
    "m.show()\n",
    "m.setShape('tri')\n",
    "m.show()\n",
    "m.setLevel(5)\n",
    "m.setShape('pyr')\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 pts)\n",
    "2). **ML Skill**\n",
    "$$ y = ax + b $$\n",
    "The above equation is your favorite linear equation where <code>a,b</code> are the constant value indicate the slope and the offset of the line in the graph.\n",
    "\n",
    "We all know given and two points. $$ (x_1,y_1) (x_2,y_2) $$ you can find <code>a,b</code> very easy using Geometry \n",
    "\n",
    "$$ a = \\frac{y_2 - y_1}{x_2 - x_1} $$\n",
    "$$ b = y_i - ax_i$$\n",
    "\n",
    "Since we have learnt that using LinearRegression can find the value of the <code>a,b</code> too.\n",
    "\n",
    "Now, do the followings.\n",
    "\n",
    "- Write a function <code>drawLine</code> that takes two tuples as inputs.\n",
    "- Calculate <code>a,b</code> using Geometry.\n",
    "- Draw the first graph with scatter on the given two points and a line.\n",
    "- Calculate <code>a,b</code> using LinearRegression with Batch Gradient Descent.\n",
    "    - Generate 1000 sample data along the line.\n",
    "    - Regress on the data using LinearRegression-BatchGradientDescent\n",
    "- Draw the second graph with scatter on the given two points and a line.\n",
    "- Does both method yeild the same outcome? Which method runs faster? (use timeit)\n",
    "- What will happen if the data is normalize first? (draw another graph and timeit)\n",
    "- What will happen if the data is standardize first? (draw another graph and timeit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (69 pts)\n",
    "\n",
    "1). **Exploratory Data Analysis**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data <code>\"howlongwelive.csv\"</code> to pandas and print the first 5 and last 5 rows of data  (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the shape, feature names, and summary (describe) of the data (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check whether there is missing data. (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fix all missing data using means or mode (1 or 0pt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Since Hepatatis B has a lot of nans, and highly correlate with Diptheria, simply drop column Hepatatis.  Also drop column Population since there are way too many nans (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If there are any features which are string and you want to use them as features, we need to convert them to int or float.  For now, convert <code>Status</code> to 0 or 1 (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Rename column <code>thinness_1-19_years</code> to <code>thinness_10-19_years</code> (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform a <code>groupby</code> country and plot their life expectancy.  Which country has the lowest/highest life expectancy? (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Plot average life expectancy of developed country vs. developing country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Perform a t-test of life expectancy between developed and developing countries.  Is the result significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Perform a <code>pairplot</code> to see which features are likely to have strong predictive power for life expectancy.  Identify the most 3 important features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Perform a histogram of life expectancy.  Is it normal? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "# boston = load_boston()\n",
    "# X = boston.data\n",
    "# y = boston.target\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **Regression**\n",
    "- Prepare your <code>X</code> and <code>y</code> into Numpy array (you have to map from Pandas to numpy).  For X, prepare two versions of them.  For first <code>X_selected</code>, you have to choose the most 3 important features from above, and for second <code>X_all</code>, simply use all features (you may want to omit Country since they are categorical). Set <code>y</code> to life expectancy. (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform standardization using Numpy way (NOT sklearn way). (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform train-test split by using Numpy way (NOT sklearn way).  Use test size of 0.3.   (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform assertion whether your splitting is correct accordingly (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a class <code>Regression(X, y, grad_method, max_iter, alpha, tol, decay, decay_iter, decay_rate, stop_delay_counter, verbose, lam, poly, poly_deg)</code> that can perform the followings:\n",
    "    - Mini-batch, Stochastic, and Batch Gradient Descent (each 2pts)\n",
    "    - Polynomial of degree k (2 or 0pt)\n",
    "    - Decay learning rate (1 or 0pt)\n",
    "        - Decay learning rate is a learning rate that becomes smaller after certian iteration. For example, after 5 iterations, the learning rate will reduce to 95% of the current learning rate.\n",
    "        - To implement it, simply multiply current learning rate with some constant <code>decay_rate</code>. For now, set it to 0.9\n",
    "    - Regularization with ridge (2 or 0pt)\n",
    "    - Must have at least four methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>score()</code> (i.e., for returning $r^2$ score), and <code>mse()</code> (return mse) (each 1pt)\n",
    "    - Accepts <code>X</code>, <code>y</code>, <code>grad_method</code> (default set to \"batch\"), <code>alpha</code> (learning rate), <code>max_iter</code>, <code>tol</code>, <code>decay</code> (whether to use decay learning rate; default set to False), <code>decay_iter</code> (after how many iterations will the decay apply), <code>stop_delay_counter</code> (this is the maximum number of times that decay the learning rate), <code>verbose</code> (default is set to False, whether model will display the Cost for each iteration), <code>lam</code> (this is the ridge regularization parameter), <code>poly</code> (default is set to False), and <code>poly_deg</code> (default is set to 2) (each 1/13pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the following 3 models **from your class** (For any unspecified parameters, feel free to use any :D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the first model, transform your feature using polynomial degree 3, then perform linear regression with batch gradient descent with early stopping of <code>tol</code> 1e-3  (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the second model, perform linear regression with mini-batch gradient desent with early stopping of <code>tol</code> 1e-3 (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the third model, perform ridge regression with stochastic gradient desent with early stopping of <code>tol</code> 1e-3 and <code>decay</code> set to True and <code>lam</code> to 1e-4 (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Lasso model from Sklearn with default parameters (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For these four models, using two different versions of X, perform a cross validation of 10 folds, comparing the four models * two versions of X.  Here you should implement cross validation. Report which one is the best candidate model (3pts for implement from scratch or 1pt for using sklearn)  \n",
    "    - Recall that in a 10 folds cross validation, you split your data into 10 even pieces.  Then you run 10 iterations, where in each iteration, you pick 1 of this piece as the validation set, and the rest as training set.  Once you reach the 10th iteration, you would have already exhaust all the 10 pieces as validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the best model, fit again with the training data.  Plot the weights using bar charts along the feature names.  Before you actually plot the weights, we need to multiply these weights by their feature standard deviation, so to reduce these weights to same unit of measure.  Interpret these weights and what they imply.  (For those who are curious why we need to multiply with std, you may read this > https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#interpreting-coefficients-scale-matters  (2 or 0pt)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the plot shows that ICR(income composition of resources), ICR^2 and AM( adult mortality) have the most impact on life expectancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform predictions on testing data.  Print adjusted $r^2$ and mse. (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the predicted values against actual values (1 or 0pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X, y, percent_train=.7):\n",
    "    idx = np.arange(0,len(X),1)\n",
    "    np.random.shuffle(idx)\n",
    "    idx_train = idx[0:int(percent_train*len(X))]\n",
    "    idx_test = idx[len(idx_train):len(idx)]\n",
    "    X_train = X[idx_train]\n",
    "    X_test = X[idx_test]\n",
    "    y_train = y[idx_train]\n",
    "    y_test = y[idx_test]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def standardScaler(data):\n",
    "    mean = data.mean() # mean stores mean value for the column\n",
    "    std = data.std() # std stores standard deviation value for the column\n",
    "    return (data - mean) / std # standard scaling of each element of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "X = standardScaler(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (70, 2)\n",
      "X_test: (30, 2)\n",
      "y_train: (70,)\n",
      "y_test: (30,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, .7)\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    # if batch, set alpha to smaller values\n",
    "    def __init__(self, X, y, grad_method=\"batch\", max_iter=10000, alpha=0.001,\n",
    "                 tol=1e-5, decay=False, decay_iter=5, decay_rate=.9, stop_delay_counter=10,\n",
    "                 verbose=False, lam=0, poly=False, poly_deg=2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.losses = []\n",
    "        self.method = grad_method\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.loss_old = 10000\n",
    "        self.tol = tol\n",
    "        self.decay = decay\n",
    "        self.decay_iter = decay_iter\n",
    "        self.decay_rate = decay_rate\n",
    "        self.stop_delay_counter = stop_delay_counter\n",
    "        self.verbose = verbose\n",
    "        self.lam = lam\n",
    "        self.poly = poly\n",
    "        self.poly_deg = poly_deg\n",
    "        \n",
    "    def fit(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        iter_stop = 0\n",
    "        list_of_used_ix = [] #<===without replacement\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            if self.method == \"sto\":\n",
    "                i = np.random.randint(X.shape[0])\n",
    "                while i in list_of_used_ix:\n",
    "                    i = np.random.randint(X.shape[0])\n",
    "                X_train = X[i, :].reshape(1, -1)\n",
    "                y_train = y[i]\n",
    "                list_of_used_ix.append(i)\n",
    "                if len(list_of_used_ix) == X.shape[0]:\n",
    "                    list_of_used_ix = []\n",
    "            elif self.method == 'minibatch':\n",
    "                batch_size = int(0.3 * X.shape[0])\n",
    "                ix = np.random.randint(0, X.shape[0]) #<----with replacement\n",
    "                X_train = X[ix:ix+batch_size]\n",
    "                y_train = y[ix:ix+batch_size]\n",
    "            else:\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "\n",
    "            loss, grad = self.gradient(X_train, y_train)\n",
    "            \n",
    "            # early stopping\n",
    "            if self.delta_loss(loss, self.loss_old, self.tol):  #np.allclose\n",
    "                iter_stop = i\n",
    "                break\n",
    "            self.loss_old = loss\n",
    "            \n",
    "            # update theta\n",
    "            self.theta = self.theta - self.alpha * grad\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # decay\n",
    "            if self.decay and self.stop_delay_counter > 0 and i % self.decay_iter == 0:\n",
    "                self.alpha = self.alpha * self.decay_rate\n",
    "                self.stop_delay_counter = self.stop_delay_counter - 1\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Loss at iteration {i}\", loss)\n",
    "\n",
    "    def predict(self, X): # <--- X_test\n",
    "        return self.h_theta(X)\n",
    "\n",
    "    # can name it predict for easy understanding\n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "    def delta_loss(self, loss_new, loss_old, tol):\n",
    "        return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        yhat = self.h_theta(X)\n",
    "        loss = self.mse(yhat, y)\n",
    "        grad = X.T @ (yhat - y)\n",
    "        return loss, grad\n",
    "    \n",
    "    def plot(self, X_test, y_test):\n",
    "        yhat = self.predict(X_test)\n",
    "        mse = self.mse(yhat, y_test)\n",
    "        print(\"MSE:\", mse)\n",
    "        print(\"R2:\", goodness_of_fit(y_test, yhat))\n",
    "#         plt.plot(self.X, self.y, 'o')\n",
    "#         plt.plot(X_test, yhat)\n",
    "#         plt.show()\n",
    "\n",
    "def goodness_of_fit(y, y_predicted):\n",
    "    # YOUR CODE HERE\n",
    "    return 1 - (np.sum((y - y_predicted.T) ** 2) / np.sum((y - y_predicted.mean()) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 335.0713291196478\n",
      "R2: 0.9386989064703603\n"
     ]
    }
   ],
   "source": [
    "m1 = Regression(X_train, y_train, verbose=False, tol=1e-3)\n",
    "m1.fit()\n",
    "m1.plot(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 331.9016122819916\n",
      "R2: 0.9392346519637957\n"
     ]
    }
   ],
   "source": [
    "m2 = Regression(X_train, y_train, grad_method='minibatch', tol=1e-3)\n",
    "m2.fit()\n",
    "m2.plot(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 334.75664392379525\n",
      "R2: 0.9387505061785516\n"
     ]
    }
   ],
   "source": [
    "m3 = Regression(X_train, y_train, grad_method='sto', tol=1e-3)\n",
    "m3.fit()\n",
    "m3.plot(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0 (0-10) of the data, MSE: 174.04291486211733.\n",
      "With 10 (10-20) of the data, MSE: 563.6984471108576.\n",
      "With 20 (20-30) of the data, MSE: 107.01519733030555.\n",
      "With 30 (30-40) of the data, MSE: 480.7103541314768.\n",
      "With 40 (40-50) of the data, MSE: 230.44825502697574.\n",
      "With 50 (50-60) of the data, MSE: 432.6935208693787.\n",
      "With 60 (60-70) of the data, MSE: 285.5539665562659.\n",
      "With 70 (70-80) of the data, MSE: 297.0863384923918.\n",
      "With 80 (80-90) of the data, MSE: 346.93192541054094.\n",
      "With 90 (90-100) of the data, MSE: 114.03559111289047.\n"
     ]
    }
   ],
   "source": [
    "def cv(X, y, cv=10):\n",
    "    m = X.shape[0]\n",
    "    foldsize = int(m/cv)\n",
    "    yhats = []\n",
    "    mses = []\n",
    "    for i in range(0, m, foldsize):\n",
    "        X_test_ = X[i:i+foldsize]\n",
    "        y_test_ = y[i:i+foldsize]\n",
    "        X_train_ = np.concatenate((X[:i], X[i+foldsize:]))\n",
    "        y_train_ = np.concatenate((y[:i], y[i+foldsize:]))\n",
    "        m = Regression(X_train_, y_train_, tol=1e-3)\n",
    "        m.fit()\n",
    "        yhat = m.predict(X_test_)\n",
    "        mse = m.mse(yhat, y_test_)\n",
    "        print(f\"With {i} ({i}-{i+foldsize}) of the data, MSE: {mse}.\")\n",
    "        yhats.append(yhat)\n",
    "        mses.append(mse)\n",
    "#     return yhats, mses\n",
    "cv(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). **Classification**\n",
    "\n",
    "- Change your y to discrete value.  Here split y into three class, {0, 1, 2}, where 0 belongs to low life expectancy group, and 2 for the high life expectancy group. (1 or 0pt)\n",
    "\n",
    "- Write a class for multinomial logistic regression with stochastic gradient descent. Must have at least six methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>accuracy()</code> (i.e., for returning accuracy score), <code>recall()</code>, <code>precision()</code>, and <code>f1()</code> (each 1pt)\n",
    "\n",
    "- Using the best X_train of the two suggested by the cross validation step, fit the data with your class.  (1 or 0pt)\n",
    "\n",
    "- Perform predictions on testing data.  Print accuracy, recall, precision, and f1_score from your class. (1 or 0pt)\n",
    "\n",
    "- Plot the decision boundary with the X_test data.  To plot this, you may want to choose only 2 features.  (1 or 0pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4). **Final verdict**\n",
    "\n",
    "- Attempt to do whatever ways - including sklearn or scratch - or change your features, or do feature enginnering such that your mse is lowest possible.  (0 to 5pts - following class normal distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error 335.4184359284476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, yhat)\n",
    "print(\"Mean Squared Error {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (105, 3)\n",
      "X_test: (45, 3)\n",
      "y_train: (105,)\n",
      "y_test: (45,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn import datasets\n",
    "import time\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the first two features.\n",
    "y = iris.target  #now our y is three classes thus require multinomial\n",
    "\n",
    "X = standardScaler(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, .7)\n",
    "\n",
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "# make sure our y is in the shape of (m, k)\n",
    "# we will convert our output vector in \n",
    "# matrix where no. of columns is equal to the no. of classes. \n",
    "# The values in the matrix will be 0 or 1. For instance the rows \n",
    "# where we have output 2 the column 2 will contain 1 and the rest are all 0.\n",
    "# in simple words, y will be of shape (m, k)\n",
    "k = len(set(y))  # no. of class  (can also use np.unique)\n",
    "m = X_train.shape[0]  # no.of samples\n",
    "n = X_train.shape[1]  # no. of features\n",
    "Y_train_encoded = np.zeros((m, k))\n",
    "for each_class in range(k):\n",
    "    cond = y_train==each_class\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "Y_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, k, n, method, alpha = 0.001, max_iter=10000):\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.W = np.random.rand(self.n, self.k)\n",
    "        self.losses = []\n",
    "        \n",
    "        if self.method == \"batch\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                loss, grad =  self.gradient(X, Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "#                 if i % 500 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        elif self.method == \"minibatch\":\n",
    "            start_time = time.time()\n",
    "            batch_size = int(0.3 * X.shape[0])\n",
    "            for i in range(self.max_iter):\n",
    "                ix = np.random.randint(0, X.shape[0]) #<----with replacement\n",
    "                batch_X = X[ix:ix+batch_size]\n",
    "                batch_Y = Y[ix:ix+batch_size]\n",
    "                loss, grad = self.gradient(batch_X, batch_Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "#                 if i % 500 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        elif self.method == \"sto\":\n",
    "            start_time = time.time()\n",
    "            list_of_used_ix = []\n",
    "            for i in range(self.max_iter):\n",
    "                idx = np.random.randint(X.shape[0])\n",
    "                while i in list_of_used_ix:\n",
    "                    idx = np.random.randint(X.shape[0])\n",
    "                X_train = X[idx, :].reshape(1, -1)\n",
    "                Y_train = Y[idx]\n",
    "                loss, grad = self.gradient(X_train, Y_train)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "                \n",
    "                list_of_used_ix.append(i)\n",
    "                if len(list_of_used_ix) == X.shape[0]:\n",
    "                    list_of_used_ix = []\n",
    "#                 if i % 500 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Method must be one of the followings: \"batch\", \"minibatch\" or \"sto\".')\n",
    "        \n",
    "        \n",
    "    def gradient(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        h = self.h_theta(X, self.W)\n",
    "        loss = - np.sum(Y*np.log(h)) / m\n",
    "        error = h - Y\n",
    "        grad = self.softmax_grad(X, error)\n",
    "        return loss, grad\n",
    "\n",
    "    def softmax(self, theta_t_x):\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return  X.T @ error\n",
    "\n",
    "    def h_theta(self, X, W):\n",
    "        '''\n",
    "        Input:\n",
    "            X shape: (m, n)\n",
    "            w shape: (n, k)\n",
    "        Returns:\n",
    "            yhat shape: (m, k)\n",
    "        '''\n",
    "        return self.softmax(X @ W)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(self.h_theta(X_test, self.W), axis=1)\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(np.arange(len(self.losses)) , self.losses, label = \"Train Losses\")\n",
    "        plt.title(\"Losses\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"losses\")\n",
    "        plt.legend()\n",
    "        \n",
    "    def classification_reports(self, actual, predicted): # <--- y_test actual\n",
    "        self.actual = actual\n",
    "        self.predicted = predicted\n",
    "        self.TP = sum((self.actual == 1) & (self.predicted == 1))\n",
    "        self.TN = sum((self.actual == 0) & (self.predicted == 0))\n",
    "        self.FN = sum((self.actual == 1) & (self.predicted == 0))\n",
    "        self.FP = sum((self.actual == 0) & (self.predicted == 1))\n",
    "        \n",
    "    def accuracy(self):\n",
    "        self.acc = 100 * (self.TP + self.TN)/ float( self.TP + self.TN + self.FN + self.FP)\n",
    "        return self.acc\n",
    "    \n",
    "    def recall(self):\n",
    "        self.recall = (100* self.TP)/ float(self.TP + self.FN)\n",
    "        return self.recall\n",
    "        \n",
    "    def precision(self):\n",
    "        self.precision = 100* (self.TP)/ float(self.TP + self.FP)\n",
    "        return self.precision\n",
    "        \n",
    "    def f1(self):\n",
    "        self.f1 = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "        return self.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n",
      "Recall: 100.0\n",
      "Precision: 100.0\n",
      "F1: 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression(k, X_train.shape[1], \"batch\")\n",
    "model.fit(X_train, Y_train_encoded)\n",
    "yhat = model.predict(X_test)\n",
    "model.classification_reports(y_test, yhat)\n",
    "print(f\"Accuracy: {model.accuracy()}\")\n",
    "print(f\"Recall: {model.recall()}\")\n",
    "print(f\"Precision: {model.precision()}\")\n",
    "print(f\"F1: {model.f1()}\")\n",
    "\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set at index 0, accuracy: 1.0\n",
      "Validation set at index 1, accuracy: 1.0\n",
      "Validation set at index 2, accuracy: 1.0\n",
      "Validation set at index 3, accuracy: 1.0\n",
      "Validation set at index 4, accuracy: 0.9\n",
      "Validation set at index 5, accuracy: 1.0\n",
      "Validation set at index 6, accuracy: 1.0\n",
      "Validation set at index 7, accuracy: 0.7\n",
      "Validation set at index 8, accuracy: 1.0\n",
      "Validation set at index 9, accuracy: 1.0\n",
      "Validation set at index 10, accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "def cv(X, y, cv=10):\n",
    "    m = X.shape[0]\n",
    "    foldsize = int(m/cv)\n",
    "    yhats = []\n",
    "    yhat_score = []\n",
    "    for i in range(0, m, foldsize):\n",
    "        X_test_ = X[i:i+foldsize]\n",
    "        y_test_ = y[i:i+foldsize]\n",
    "        X_train_ = np.concatenate((X[:i], X[i+foldsize:]))\n",
    "        y_train_ = np.concatenate((y[:i], y[i+foldsize:]))\n",
    "        \n",
    "        # add intercept to our X\n",
    "        intercept = np.ones((X_train_.shape[0], 1))\n",
    "        X_train_   = np.concatenate((intercept, X_train_), axis=1)  #add intercept\n",
    "        intercept = np.ones((X_test_.shape[0], 1))\n",
    "        X_test_    = np.concatenate((intercept, X_test_), axis=1)  #add intercept\n",
    "        \n",
    "        Y_train_encoded_ = np.zeros((X_train_.shape[0], k))\n",
    "        for each_class in range(k):\n",
    "            cond = y_train_==each_class\n",
    "            Y_train_encoded_[np.where(cond), each_class] = 1\n",
    "        model = LogisticRegression(k, X_train_.shape[1], \"batch\")\n",
    "        model.fit(X_train_, Y_train_encoded_)\n",
    "        yhat = model.predict(X_test_)\n",
    "        acc = (yhat==y_test_).sum() / y_test_.shape[0]\n",
    "        yhats.append(yhat)\n",
    "        yhat_score.append(acc)\n",
    "    return yhats, yhat_score\n",
    "\n",
    "yhats, accs = cv(X_train, y_train)\n",
    "for i, acc in enumerate(accs):\n",
    "    print(f\"Validation set at index {i}, accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (95, 4)\n",
      "X_test: (10, 4)\n",
      "y_train: (95,)\n",
      "y_test: (10,)\n",
      "Accuracy: 100.0\n",
      "Recall: 100.0\n",
      "Precision: 100.0\n",
      "F1: 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAku0lEQVR4nO3de5wU9Znv8c/Tl7nADBdhuAgooMSICqijKGqikvWSaMxRY6JGUZM1JjEmm914ydWYZJNdT26emCUeV01Wo/GamOiGHI0xMV7BBQQJgigwoDLDdXCYmZ7u5/xRNU3P0AwDTE0PU9/369WvrvpVVffzG6W//auqrjJ3R0RE4itR6gJERKS0FAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEEjsmdmbZvaBUtchUioKAhGRmFMQiBRhZuVm9mMzWxs+fmxm5eGy4Wb2ezPbZGYbzOyvZpYIl11nZmvMrNHMlprZzLA9YWbXm9nrZrbezO43s/3CZRVmdnfYvsnMXjKzkaXrvcSNgkCkuK8CxwHTgKnAscDXwmX/DNQBNcBI4CuAm9khwNXAMe5eDZwOvBlucw3wEeD9wP7ARuDWcNksYDAwDhgGXAVsi6pjIp0pCESKuxi4yd3XuXs98C3gknBZBhgNHOjuGXf/qwcX7coC5cBkM0u7+5vu/nq4zaeBr7p7nbu3ADcC55tZKny9YcDB7p5193nuvqXXeiqxpyAQKW5/YGXB/MqwDeBmYDnwRzNbYWbXA7j7cuCLBB/y68zsPjNr3+ZA4JFw188mYAlBcIwE/guYA9wX7ob6dzNLR9k5kUIKApHi1hJ8eLc7IGzD3Rvd/Z/dfSJwNvCl9mMB7v4rdz8x3NaBfwu3Xw2c6e5DCh4V7r4mHFV8y90nAzOAs4BLe6WXIigIRNqlw4O2FWZWAdwLfM3MasxsOPAN4G4AMzvLzA42MwO2EHyzz5rZIWZ2anhQuZlgP382fP3ZwHfN7MDwNWrM7Jxw+hQzO8LMkuHrZQq2E4mcgkAk8DjBB3f7owKYCywEXgFeBr4TrjsJeALYCjwH/Mzd/0xwfOD7QAPwNjCC4EAywE+ARwl2JzUCzwPTw2WjgAcJQmAJ8DRh6Ij0BtONaURE4k0jAhGRmFMQiIjEnIJARCTmIgsCM7vDzNaZ2aKdLL/YzBaGj2fNbGpUtYiIyM5FdrDYzN5HcFbFL9398CLLZwBL3H2jmZ0J3Oju0zuv19nw4cN9/PjxPV6viEh/Nm/evAZ3rym2LBXVm7r7X8xsfBfLny2YfR4Y253XHT9+PHPnzt3L6kRE4sXMVu5sWV85RvBJ4L93ttDMrjSzuWY2t76+vhfLEhHp/0oeBGZ2CkEQXLezddz9NnevdffampqiIxsREdlDke0a6g4zmwLcTnANlvWlrEVEJK5KFgRmdgDwMHCJu79WqjpEpOdlMhnq6upobm4udSmxU1FRwdixY0mnu38B28iCwMzuBU4GhptZHfBNIA3g7rMJLuI1DPhZcO0u2ty9Nqp6RKT31NXVUV1dzfjx4wn/fUsvcHfWr19PXV0dEyZM6PZ2UZ41dOEuln8K+FRU7y8ipdPc3KwQKAEzY9iwYezuSTUlP1gsIv2TQqA09uTvHpsgWPp2Iz/441IatraUuhQRkT4lNkGwfN1W/s+flrPh3dZSlyIiEVu/fj3Tpk1j2rRpjBo1ijFjxuTnW1u7/gyYO3cu11xzzW693/jx42loaNibkkuqpKeP9qb20VJO918Q6feGDRvG/PnzAbjxxhupqqriX/7lX/LL29raSKWKf/zV1tZSWxuv81ZiMyJIhEGgHBCJp8suu4wvfelLnHLKKVx33XW8+OKLzJgxgyOPPJIZM2awdOlSAP785z9z1llnAUGIXHHFFZx88slMnDiRW265pdvvt3LlSmbOnMmUKVOYOXMmq1atAuCBBx7g8MMPZ+rUqbzvfe8DYPHixRx77LFMmzaNKVOmsGzZMgDuvvvufPunP/1pstks2WyWyy67jMMPP5wjjjiCH/3oR3v9t4nNiACCJNCIQKR3fet3i3l17ZYefc3J+w/im2cfttvbvfbaazzxxBMkk0m2bNnCX/7yF1KpFE888QRf+cpXeOihh3bY5u9//ztPPfUUjY2NHHLIIXzmM5/p1jn6V199NZdeeimzZs3ijjvu4JprruE3v/kNN910E3PmzGHMmDFs2rQJgNmzZ/OFL3yBiy++mNbWVrLZLEuWLOHXv/41f/vb30in03z2s5/lnnvu4bDDDmPNmjUsWhRc2Ln9NfZGbIJAIwIR+ehHP0oymQRg8+bNzJo1i2XLlmFmZDKZott86EMfory8nPLyckaMGME777zD2LG7vkbmc889x8MPPwzAJZdcwrXXXgvACSecwGWXXcYFF1zAueeeC8Dxxx/Pd7/7Xerq6jj33HOZNGkSTz75JPPmzeOYY44BYNu2bYwYMYKzzz6bFStW8PnPf54PfehDnHbaaXv9d4lNELSfUqUgEOlde/LNPSoDBw7MT3/961/nlFNO4ZFHHuHNN9/k5JNPLrpNeXl5fjqZTNLW1rZH793+GTR79mxeeOEFHnvsMaZNm8b8+fO56KKLmD59Oo899hinn346t99+O+7OrFmz+N73vrfDay1YsIA5c+Zw6623cv/993PHHXfsUU3t4neMACWBiAQjgjFjxgBw11139fjrz5gxg/vuuw+Ae+65hxNPPBGA119/nenTp3PTTTcxfPhwVq9ezYoVK5g4cSLXXHMNH/7wh1m4cCEzZ87kwQcfZN26dQBs2LCBlStX0tDQQC6X47zzzuPb3/42L7/88l7XGqMRQfCcUw6ICHDttdcya9YsfvjDH3Lqqafu9etNmTKFRCL4bn3BBRdwyy23cMUVV3DzzTdTU1PDnXfeCcCXv/xlli1bhrszc+ZMpk6dyve//33uvvtu0uk0o0aN4hvf+Ab77bcf3/nOdzjttNPI5XKk02luvfVWKisrufzyy8nlcgBFRwy7K7I7lEWltrbW9+TGNE8tXcfld77EI5+dwZEHDI2gMhFpt2TJEg499NBSlxFbxf7+ZjZvZ9dzi82uofYfXWtEICLSUWyCIJG//oaSQESkUGyCQMcIRHrXvrbbub/Yk797fIIAnT4q0lsqKipYv369wqCXtd+PoKKiYre2i81ZQwlda0ik14wdO5a6urrdvi6+7L32O5TtjtgEAfplsUivSafTu3WHLCmt2Owaaj9YrB+UiYh0FJsgyJ8zpBwQEekgNkGQSOhgsYhIMbEJgu0/KFMSiIgUik8Q5I8RiIhIoRgFQfCsEYGISEexCYL8JSaUAyIiHcQmCHSMQESkuNgEQUJ3KBMRKSo2QaBjBCIixcUuCBQDIiIdxScI8lcfVRSIiBSKLAjM7A4zW2dmi3ay3MzsFjNbbmYLzeyoqGoBCG8lqmMEIiKdRDkiuAs4o4vlZwKTwseVwH9EWEt+RKAb04iIdBRZELj7X4ANXaxyDvBLDzwPDDGz0VHVk8gfI1ASiIgUKuUxgjHA6oL5urBtB2Z2pZnNNbO5e3qjC92qUkSkuFIGgRVpK/ox7e63uXutu9fW1NTs2ZuZDhaLiBRTyiCoA8YVzI8F1kb1ZrofgYhIcaUMgkeBS8Ozh44DNrv7W1G9me5QJiJSXGT3LDaze4GTgeFmVgd8E0gDuPts4HHgg8ByoAm4PKpagnqC51wuyncREdn3RBYE7n7hLpY78Lmo3r+zhO5HICJSVGx+WdxO1xoSEekoNkGQSOhiQyIixcQmCHQ/AhGR4uITBBoQiIgUFZsgaD9YrBGBiEhHsQkC/aBMRKS4+ASBTh8VESkqRkEQPOtaQyIiHcUmCHTzehGR4mITBDp9VESkuNgEgUYEIiLFxSYIyN+YRkkgIlIoNkGQKHYbHBERiU8QmH5QJiJSVGyCIH/NOeWAiEgHsQkCo31EUOJCRET6mPgEgQ4Wi4gUFZsg2H76qIJARKRQbIIgFR4kyOqexSIiHcQmCBL5IFASiIgUik0QACQTRla7hkREOohXEJhp15CISCfxCoKE6awhEZFOYhcEbVkFgYhIoVgFQcL0OwIRkc5iFQSpZIKsflosItJBrIIgYTprSESks1gFQTIBWR0jEBHpINIgMLMzzGypmS03s+uLLB9sZr8zswVmttjMLo+ynqRGBCIiO4gsCMwsCdwKnAlMBi40s8mdVvsc8Kq7TwVOBn5gZmVR1ZRMGjkdIxAR6SDKEcGxwHJ3X+HurcB9wDmd1nGg2oK7xlQBG4C2qArSiEBEZEdRBsEYYHXBfF3YVuinwKHAWuAV4AvuvsNvf83sSjOba2Zz6+vr97igRMJ01pCISCdRBkGxuwR3/hQ+HZgP7A9MA35qZoN22Mj9NnevdffampqaPS4ouMSEgkBEpFCUQVAHjCuYH0vwzb/Q5cDDHlgOvAG8N6qCkhoRiIjsIMogeAmYZGYTwgPAHwce7bTOKmAmgJmNBA4BVkRVkK41JCKyo1RUL+zubWZ2NTAHSAJ3uPtiM7sqXD4b+DZwl5m9QrAr6Tp3b4iqJo0IRER2FFkQALj748DjndpmF0yvBU6LsoZCyYTRpiAQEekgXr8sNu0aEhHpLFZBoNNHRUR2FKsgSJqhWxaLiHQUqyBIJY02JYGISAexCoLgMtSlrkJEpG+JVRAkE7ronIhIZ7ELAh0sFhHpKF5BoGsNiYjsIF5BkNDBYhGRzmIVBOmkflksItJZrIKgLJUg06YRgYhIoVgFQTqZoFXnj4qIdBC7IMhkNSIQESm020FgZkPNbEoUxUStLKUgEBHprFtBYGZ/NrNBZrYfsAC408x+GG1pPS+dNAWBiEgn3R0RDHb3LcC5wJ3ufjTwgejKikawa8j162IRkQLdDYKUmY0GLgB+H2E9kUong+5m9FsCEZG87gbBTQS3nHzd3V8ys4nAsujKikZZexDozCERkbxu3arS3R8AHiiYXwGcF1VRUSlLhUHQloPyEhcjItJHdPdg8XvM7EkzWxTOTzGzr0VbWs/L7xrSAWMRkbzu7hr6v8ANQAbA3RcCH4+qqKikkwZAq4JARCSvu0EwwN1f7NTW1tPFRC2/a0jHCERE8robBA1mdhDgAGZ2PvBWZFVFRLuGRER21K2DxcDngNuA95rZGuAN4BORVRWR9iBo1YXnRETyunvW0ArgA2Y2EEi4e2O0ZUVDxwhERHbU3bOGvmBmg4Am4Edm9rKZnRZtaT0v/zsCjQhERPK6e4zgivASE6cBI4DLge9HVlVE2g8WtygIRETyuhsEFj5/kOBaQwsK2vYZFekkAM2ZbIkrERHpO7obBPPM7I8EQTDHzKqBfe5r9YCyIAi2KQhERPK6GwSfBK4HjnH3JiBNsHuoS2Z2hpktNbPlZnb9TtY52czmm9liM3u625Xvgcr2IGhVEIiItOvu6aPHA/Pd/V0z+wRwFPCTrjYwsyRwK/APQB3wkpk96u6vFqwzBPgZcIa7rzKzEXvQh26rDHcNNSkIRETyujsi+A+gycymAtcCK4Ff7mKbY4Hl7r7C3VuB+4BzOq1zEfCwu68CcPd13a58D1Rq15CIyA66GwRt7u4EH+Q/cfefANW72GYMsLpgvi5sK/QeYGh4B7R5ZnZpsRcysyvNbK6Zza2vr+9myTsqSyZImHYNiYgU6u6uoUYzuwG4BDgp3O2T3sU2xc4q6nyRnxRwNDATqASeM7Pn3f21Dhu530bwy2Zqa2v3+EJBZkZlOqkRgYhIge6OCD4GtBD8nuBtgm/2N+9imzpgXMH8WGBtkXX+4O7vunsD8Bdgajdr2iOVZSkFgYhIgW4FQfjhfw8w2MzOAprdfVfHCF4CJpnZBDMrI7hs9aOd1vktwQgjZWYDgOnAkt3qwW6qLEto15CISIHuXmLiAuBF4KME9y1+IbwC6U65extwNcEtLpcA97v7YjO7ysyuCtdZAvwBWBi+/u3uvmhPO9MdlemkgkBEpEB3jxF8leA3BOsAzKwGeAJ4sKuN3P1x4PFObbM7zd/Mrncz9ZjKshRN2jUkIpLX3WMEiU6ndq7fjW37lMp0gmaNCERE8ro7IviDmc0B7g3nP0anb/r7iqryFGs2NZe6DBGRPqO79yP4spmdB5xAcFrobe7+SKSVRWRQRZol2/bJ2ymIiESiuyMC3P0h4KEIa+kVgyrTbGnOlLoMEZE+o8sgMLNGdvwRGASjAnf3QZFUFaFBlWm2trSRyzmJxD53JW0RkR7XZRC4+64uI7HPGVSRwh0aW9oYXLmrH0eLiPR/++SZP3tjUPjhv2Wbdg+JiEAMg6B9FLBZQSAiAsQwCAZVhCMCHTAWEQHiGASVwWGRLdvaSlyJiEjfELsg2L5rqLXElYiI9A2xC4JhA8sBaNiqIBARgRgGQWVZkqryFPWNLaUuRUSkT4hdEADUVJfTsFVBICICcQ2CqnKNCEREQvEMgupy6jUiEBEBYhoEw6vKaNCIQEQEiGkQ1FSXs6W5jWbdqUxEJJ5BMGJQBQDvbNENakREYhkEY4dWAlC3cVuJKxERKb1YBsG4oQMAqNvYVOJKRERKL5ZBMHpwBcmEsXqDRgQiIrEMglQywejBFRoRiIgQ0yCA4DjBah0jEBGJbxCMGzqAVRs0IhARiW0QTKypor6xRXcqE5HYi20QvGdkFQDL1zWWuBIRkdKKbRBMGlENwLJ3tpa4EhGR0oo0CMzsDDNbambLzez6LtY7xsyyZnZ+lPUUGju0ksp0ktcUBCISc5EFgZklgVuBM4HJwIVmNnkn6/0bMCeqWopJJIyDR1SxTLuGRCTmohwRHAssd/cV7t4K3AecU2S9zwMPAesirKWoQ0ZVs+StLbh7b7+1iEifEWUQjAFWF8zXhW15ZjYG+F/A7K5eyMyuNLO5Zja3vr6+xwqcOnYwDVtbWbNJvycQkfiKMgisSFvnr94/Bq5z9y6vB+3ut7l7rbvX1tTU9FR9TB03BIAFqzf32GuKiOxrogyCOmBcwfxYYG2ndWqB+8zsTeB84Gdm9pEIa+rgvaMGUZZMsKBuU2+9pYhIn5OK8LVfAiaZ2QRgDfBx4KLCFdx9Qvu0md0F/N7dfxNhTR2UpRJM3n8Q81dt6q23FBHpcyIbEbh7G3A1wdlAS4D73X2xmV1lZldF9b676+gDh7KgbpPuViYisRXliAB3fxx4vFNb0QPD7n5ZlLXszIyDhvGfz7zByys3MuPg4aUoQUSkpGL7y+J20ycOI5kwnlneUOpSRERKIvZBUFWeYtq4Ifzt9fWlLkVEpCRiHwQAJxw8nFfqNrHh3dZSlyIi0usUBMBpk0eSc3ji1XdKXYqISK9TEACH7T+IMUMq+cPit0tdiohIr1MQAGbGGYeP4pllDTQ260Y1IhIvCoLQB48YRWs2x5zF2j0kIvGiIAgddcBQJgwfyP0vrd71yiIi/YiCIGRmXFA7jhff3MCKet2sRkTiQ0FQ4Lyjx5BMGPdpVCAiMaIgKDCiuoIzDh/FvS+s0kFjEYkNBUEnn37fRBpb2vjVC6tKXYqISK9QEHQyZeyQ/IXodEVSEYkDBUERV596MOsaW/jFs2+WuhQRkcgpCIqYcdBwTjmkhp8+tZyNuv6QiPRzCoKduP7MQ3m3pY0fP/FaqUsREYmUgmAnDhlVzSXHHcgvn1/JvJUbS12OiEhkFARd+PIZ72X0oAque2ihDhyLSL+lIOhCVXmKfz33CJav28q/Pr6k1OWIiERCQbALJx8ygn88aQK/fG4lv1uwttTliIj0OAVBN1x7xns5+sChXPfQQhat2VzqckREepSCoBvSyQQ/u/gohg4o4/K7XmL1hqZSlyQi0mMUBN00clAFd11+DC2ZLJfe8SJvbd5W6pJERHqEgmA3TBpZzZ2XH0N9YwsX/Pw5jQxEpF9QEOymow/cj3s+NZ0t29r46OzndMxARPZ5CoI9MHXcEO678jgSBufPfpbHFr5V6pJERPaYgmAPHTp6EL+9+kQO238wn/vVy9z46GL96ExE9kkKgr1QU13Or/5xOpefMJ67nn2TD//0Ge0qEpF9joJgL5Wnknzz7MP4xRXHsrEpw4d/+gzf+t1itugOZyKyj4g0CMzsDDNbambLzez6IssvNrOF4eNZM5saZT1Rev97anjin97PRdMP4K5n32TmD57m3hdXkcnmSl2aiEiXIgsCM0sCtwJnApOBC81scqfV3gDe7+5TgG8Dt0VVT28YPCDNdz5yBL/93AmMHVrJDQ+/wswfPM3DL9eRzXmpyxMRKSrKEcGxwHJ3X+HurcB9wDmFK7j7s+7efo3n54GxEdbTa6aMHcLDn5nBf86qZWB5ii/dv4D33/wUt/91BY3aZSQifUyUQTAGWF0wXxe27cwngf8utsDMrjSzuWY2t76+vgdLjI6ZMfPQkTz2+ROZ/YmjGT24gu88toTjv/cnvvnbRSxasxl3jRJEpPRSEb62FWkr+slnZqcQBMGJxZa7+22Eu41qa2v3qU/PRMI44/BRnHH4KBbWbeKOZ97g3pdW84vnVvLeUdWcf/RYzp66PyMHVZS6VBGJKYvqW6mZHQ/c6O6nh/M3ALj79zqtNwV4BDjT3Xd5X8ja2lqfO3duBBX3ns1NGX63cC0Pzqtj/upNABx5wBBOP2wUpx82ignDB5a2QBHpd8xsnrvXFl0WYRCkgNeAmcAa4CXgIndfXLDOAcCfgEvd/dnuvG5/CIJCy9dtZc7it/nDord5JfwNwkE1AzlpUg0nHjyc4w4aRlV5lAM3EYmDkgRB+MYfBH4MJIE73P27ZnYVgLvPNrPbgfOAleEmbTsrtF1/C4JCdRub+OPid/jza/W8+MZ6mjM5UgnjyAOGMH3CMI4+cChHHjCEIQPKSl2qiOxjShYEUejPQVCoOZPl5VUbeWZZA88sb2Dx2i35U1APqhkYhsJQDt9/MJNGVlGRTpa4YhHpyxQE/UBTaxsLVm/m5VUbeXnlRuat2simpuBU1FTCOHhEFZNHD2Ly/sFj0ohqhleVYVbsmL2IxE1XQaCdz/uIAWUpjj9oGMcfNAwAd2fl+iZefWsLr67dwuK1m/nb6w08/D9r8tsMqkhx8IgqDqqp4qDw+eARVYwdWkk6qauLiEhAI4J+pmFrC0ve2sLydVt5vX4rr697l9frt7KusSW/TsJg9OBKxu1XybihAxi33wAO2G9Afr6mulwjCZF+RiOCGBleVc5Jk2o4aVJNh/bN2zKsqN/K8nVbWbWhidUbmli9cRtPv1bfISQAylIJRg2qCB6Dg8fIQRWMDp9HDa5gRHW5RhUi/YSCICYGV6Y58oDgAHNnzZksdRubWL1hG6s3NlG3cRtvb27m7c3NzF+9ibcXN9Pa1vHieWYwbGA5w6vKGF5VzrCqMoYNDJ63t5UzbGAwXVmmg9kifZWCQKhIJzl4RDUHj6guutzd2dSU4a3NzbyzpZm3twQhsa6xmYatrazf2sLq1U2s39rK1pa2oq8xoCzJ0AFlDBmQDh6VZQwekGZIZZrBlUHb4MqOy4cMSOtsKJFeoCCQXTIzhg4sY+jAMibvP6jLdZszWda/20pDYwvr320Jg6KVhq0tbGrKsHlbK5uaMizd0pifz2R3fpyqLJVgUEWK6oo0VeUpqspTVFekqKpIUV0etldsb6+uSFFVng6fU/nnlHZjieyUgkB6VEU6yZghlYwZUtmt9d2dptYsm7Zl2NTUyuamTDidYdO2YL6xpY3G5ja2NmfY2tLGqg1NNDa30RjOd+cK32XJBJVlSQbkH6ni0+UpBqSTVJYlGVgetFemg+n27SvTSSrSSSpSScrTCcpTCR1cl32agkBKyswYWJ5iYHmq2+FRyN3ZlsmGwdDG1pYwIML5Lc0Zmlqz4aONptYs21qzvBtON2xt5d3WpqCtpY1tmWyXI5SdKU8lgnBIJ/IhUZFOUJ4PjXC683rpZMG2wXRZ+ChPbp8uSyUoSyZIJxMd1ilLJjTakb2mIJB9mpmF3+hTjOx6r1W3tbbl2NaapSkThEVTSxgimWC6OZOluS1LcyZHcyZLSyZLc1sw3ZzJ0pKfDp43b8uwLlzWnMnR3JalJXzuibO3EwbpMDTKw3BoD4r29rLC5e3tyc6BYqQSCdJJIxWGTjpsSyWNdNJIJxOd1mlvC5/D+XS4TSpppBMJ0qnt6yQTGj31NQoCkU7aPxwHk470fdyd1myO5kwuCJMwHFrbcrRmc8Fz+yNbZHpXbeF8JpujpS1HY3Mb6wvWyRSsm8nlyGS9V+6kZ2FwpRPbwySVSJBOWT5AkokgOJIJ2/5c0J6wsD1pndbbcbsOy5KdXjNhJLrYtv09k1Y4X7h9Iv8eSTMSCfLT7e2JgvlE/pk+tTtRQSBSImZGeSpJeSoJldGGTnflck5bzmnL5ci0OZlcjrask8nmaMsFz5ls0NYWhkf7fMd1nLZsjkwufM63dd6ucB0veG0n58FzNqyhJZOjLZclm9vens15wXzBczaXn8+579HuvqglDFKJRBAeFgZGPlA6hkl7eFx47AF86qSJPV6LgkBE8hIJoyxhlJGAfnaR21yHwMjtECC5goBpywWhle20TS5H0W2DUAreI+ueD6hsGETZsD2Xc7I58tNthcsLpre3Eby2B689vKo8kr+NgkBEYqE95AL6fUohnW4gIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYm6fu2exmdUDK/dw8+FAQw+Wsy9Qn+NBfY6Hvenzge5eU2zBPhcEe8PM5u7s5s39lfocD+pzPETVZ+0aEhGJOQWBiEjMxS0Ibit1ASWgPseD+hwPkfQ5VscIRERkR3EbEYiISCcKAhGRmItNEJjZGWa21MyWm9n1pa5nT5nZODN7ysyWmNliM/tC2L6fmf0/M1sWPg8t2OaGsN9Lzez0gvajzeyVcNkt1pduolqEmSXN7H/M7PfhfL/us5kNMbMHzezv4X/v42PQ538K/79eZGb3mllFf+uzmd1hZuvMbFFBW4/10czKzezXYfsLZjZ+l0W5e79/ENyO6HVgIsEN+BYAk0td1x72ZTRwVDhdDbwGTAb+Hbg+bL8e+LdwenLY33JgQvh3SIbLXgSOBwz4b+DMUvdvF33/EvAr4PfhfL/uM/AL4FPhdBkwpD/3GRgDvAFUhvP3A5f1tz4D7wOOAhYVtPVYH4HPArPD6Y8Dv95lTaX+o/TSH/54YE7B/A3ADaWuq4f69lvgH4ClwOiwbTSwtFhfgTnh32M08PeC9guBn5e6P130cyzwJHAq24Og3/YZGBR+KFqn9v7c5zHAamA/gtvo/h44rT/2GRjfKQh6rI/t64TTKYJfIltX9cRl11D7/2Dt6sK2fVo45DsSeAEY6e5vAYTPI8LVdtb3MeF05/a+6sfAtUCuoK0/93kiUA/cGe4Ou93MBtKP++zua4D/DawC3gI2u/sf6cd9LtCTfcxv4+5twGZgWFdvHpcgKLZ/cJ8+b9bMqoCHgC+6+5auVi3S5l209zlmdhawzt3ndXeTIm37VJ8JvskdBfyHux8JvEuwy2Bn9vk+h/vFzyHYBbI/MNDMPtHVJkXa9qk+d8Oe9HG3+x+XIKgDxhXMjwXWlqiWvWZmaYIQuMfdHw6b3zGz0eHy0cC6sH1nfa8Lpzu390UnAB82szeB+4BTzexu+nef64A6d38hnH+QIBj6c58/ALzh7vXungEeBmbQv/vcrif7mN/GzFLAYGBDV28elyB4CZhkZhPMrIzgAMqjJa5pj4RnBvwnsMTdf1iw6FFgVjg9i+DYQXv7x8MzCSYAk4AXw+Fno5kdF77mpQXb9CnufoO7j3X38QT/7f7k7p+gf/f5bWC1mR0SNs0EXqUf95lgl9BxZjYgrHUmsIT+3ed2PdnHwtc6n+DfS9cjolIfNOnFgzMfJDjD5nXgq6WuZy/6cSLBMG8hMD98fJBgH+CTwLLweb+Cbb4a9nspBWdPALXAonDZT9nFAaW+8ABOZvvB4n7dZ2AaMDf8b/0bYGgM+vwt4O9hvf9FcLZMv+ozcC/BMZAMwbf3T/ZkH4EK4AFgOcGZRRN3VZMuMSEiEnNx2TUkIiI7oSAQEYk5BYGISMwpCEREYk5BICIScwoCkV5kZidbePVUkb5CQSAiEnMKApEizOwTZvaimc03s59bcC+ErWb2AzN72cyeNLOacN1pZva8mS00s0faryVvZgeb2RNmtiDc5qDw5ats+30G7ulL18qXeFIQiHRiZocCHwNOcPdpQBa4GBgIvOzuRwFPA98MN/klcJ27TwFeKWi/B7jV3acSXDPnrbD9SOCLBNean0hwLSWRkkmVugCRPmgmcDTwUvhlvZLgImA54NfhOncDD5vZYGCIuz8dtv8CeMDMqoEx7v4IgLs3A4Sv96K714Xz8wmuTf9M5L0S2QkFgciODPiFu9/QodHs653W6+r6LF3t7mkpmM6if4dSYto1JLKjJ4HzzWwE5O8neyDBv5fzw3UuAp5x983ARjM7KWy/BHjag3tE1JnZR8LXKDezAb3ZCZHu0jcRkU7c/VUz+xrwRzNLEFwl8nMEN4c5zMzmEdz16WPhJrOA2eEH/Qrg8rD9EuDnZnZT+Bof7cVuiHSbrj4q0k1mttXdq0pdh0hP064hEZGY04hARCTmNCIQEYk5BYGISMwpCEREYk5BICIScwoCEZGY+/8KnNN44n892wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "foldsize = int(X_train.shape[0]/10)\n",
    "X_test_ = X_train[i:i+foldsize]\n",
    "y_test_ = y_train[i:i+foldsize]\n",
    "X_train_ = np.concatenate((X_train[:i], X_train[i+foldsize:]))\n",
    "y_train_ = np.concatenate((y_train[:i], y_train[i+foldsize:]))\n",
    "\n",
    "# add intercept to our X\n",
    "intercept = np.ones((X_train_.shape[0], 1))\n",
    "X_train_   = np.concatenate((intercept, X_train_), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test_.shape[0], 1))\n",
    "X_test_    = np.concatenate((intercept, X_test_), axis=1)  #add intercept\n",
    "\n",
    "print(\"X_train:\", X_train_.shape)\n",
    "print(\"X_test:\", X_test_.shape)\n",
    "print(\"y_train:\", y_train_.shape)\n",
    "print(\"y_test:\", y_test_.shape)\n",
    "\n",
    "k = len(set(y_train))  # no. of class  (can also use np.unique)\n",
    "m = X_train_.shape[0]  # no.of samples\n",
    "n = X_train_.shape[1]  # no. of features\n",
    "Y_train_encoded_ = np.zeros((X_train_.shape[0], k))\n",
    "for each_class in range(k):\n",
    "    cond = y_train_==each_class\n",
    "    Y_train_encoded_[np.where(cond), each_class] = 1\n",
    "\n",
    "    \n",
    "model_ = LogisticRegression(k, X_train_.shape[1], \"batch\")\n",
    "model_.fit(X_train_, Y_train_encoded_)\n",
    "yhat_ = model_.predict(X_test_)\n",
    "model_.classification_reports(y_test_, yhat_)\n",
    "print(f\"Accuracy: {model_.accuracy()}\")\n",
    "print(f\"Recall: {model_.recall()}\")\n",
    "print(f\"Precision: {model_.precision()}\")\n",
    "print(f\"F1: {model_.f1()}\")\n",
    "\n",
    "model_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6429047   2.01394597 -2.25863793]\n",
      " [-7.27680659  0.85206455  7.99189194]\n",
      " [-3.84226049 -2.62382732  8.29966085]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def boundary_points(X, theta):\n",
    "    v_orthogonal = np.array([[theta[1,0]],[theta[2,0]]])\n",
    "    v_ortho_length = np.sqrt(v_orthogonal.T @ v_orthogonal)\n",
    "    dist_ortho = theta[0,0] / v_ortho_length\n",
    "    v_orthogonal = v_orthogonal / v_ortho_length\n",
    "    v_parallel = np.array([[-v_orthogonal[1,0]],[v_orthogonal[0,0]]])\n",
    "    projections = X @ v_parallel\n",
    "    proj_1 = min(projections)\n",
    "    proj_2 = max(projections)\n",
    "    point_1 = proj_1 * v_parallel - dist_ortho * v_orthogonal\n",
    "    point_2 = proj_2 * v_parallel - dist_ortho * v_orthogonal\n",
    "    return point_1, point_2\n",
    "\n",
    "class0 = (y_test==0).flatten()\n",
    "class1 = (y_test==1).flatten()\n",
    "plt.plot(X_test[class0, 1], X_test[class0, 2], 'ro')\n",
    "plt.plot(X_test[class1, 1], X_test[class1, 2], 'bo')\n",
    "print(model.W)\n",
    "point_1, point_2 = boundary_points(X, model.W)\n",
    "plt.plot([point_1[0,0], point_2[0,0]],[point_1[1,0], point_2[1,0]], 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
